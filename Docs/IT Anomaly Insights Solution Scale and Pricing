# IT Anomaly Insights Solution Scale and Pricing

The solution deployed into your subscription consists of a number of Azure resources as described in

IT Anomaly Insights Solution [document](https://github.com/Azure/itanomalyinsights-cortana-intelligence-preconfigured-solution/blob/master/Docs/IT%20Anomaly%20Insights%20Post%20Deployment%20Instructions.md#it-anomaly-insights-solution) , which determines scale supported by the solution and also contribute to total cost of the deployed solution.

## Scale Limits

The deployed solution consists of a number of Azure resources and each resource has certain scale limitation based on its performance tier. Let&#39;s take a closer look into how data is processed by each of these pipeline components; define the scale unit and lastly understand scale limits of the solution.

### Data Flow

The solution ingests events via event hub which is consumed by downstream components like Azure Stream Analytics which aggregates events by event identifier or metric. Each individual metric is from then on treated as a single time series. The Stream Analytics job aggregates events per metric and writes latest data into Azure Table storage. Azure Data Factory reads from Azure Table and fetches time series data for each metric. Each time series is then sent to be scored by the Anomaly Detection Machine Learning API and finally, results are written to Azure SQL database.

Overall, at every step, components in solution process data on a per metric basis and hence we use number of unique metrics in data as a way to measure scalability of solution.

### Recommendation

For near real time scenarios where the pipeline is running as frequently as possible, we recommend not have more than 5000 unique metrics in data. This is to ensure that the data can be processed well in time before the next execution begins such that the results are available every 15 minutes.

For scaling beyond recommended limit, please email [adpcs\_support@microsoft.com](mailto:adpcs_support@microsoft.com) to explore other options.

## Estimated Cost

The cost of running a solution depends on per service rate for each of the services listed below and also depends on factors like frequency at which the results are being generated by the solution.

### Azure Services

1. Storage account [Total: 1, Performance: Standard LRS]

2. Azure Event Hubs [Total: 1, Pricing tier: Standard]

3. Azure Stream Analytics [Total: 1, 1 Steaming unit]

4. Azure SQL Database [Total: 1, Pricing tier: Standard S1]

5. Azure Service Bus Topics [Total: 1, Pricing tier: Standard]

6. Azure Data Factory [Total: 1]

7. Application Insights [Total: 1, Pricing tier: Standard]

8. Azure Web Jobs [Total: 1, Pricing tier: Standard S1]

Below table gives a price estimation for various workload profiles.

| Pipeline frequency | Estimated Monthly Cost ($) |
| --- | --- |
| Near real time (every 15 minutes) | **~3000** |
| Hourly | **~3000** |
| Daily | **~300** \*ADF on-demand HDInsight cluster time to live set to 1 hr. See section Optimizing Resource Utilization for more details. |
| Weekly | **~200** \*ADF on-demand HDInsight cluster time to live set to 1 hr. See sectionOptimizing Resource Utilization for more details. |

Note: Prices are estimates and are not intended as actual price quotes.

# Optimizing Resource Utilization

Depending on the scenario for which the solution is used, usage may vary in terms of volume of incoming events and/or frequency at which the results are expected. Based on your scenario, solution can be customized further as below.

## Compute Resource Settings

### Time to live for on-demand HDInsight Cluster

Azure Data Factory in the solution uses on-demand HDInsight cluster that is deployed to execute pipeline activities and deleted if there are no other active jobs. The &#39;timetolive&#39; setting determines the idle time for this cluster. Typically, it takes more than 15 minutes to provision a cluster. Hence, in case of near real time scenario, we try to minimize the provisioning time by increasing the &#39;timetolive&#39; for this cluster to a larger enough value like 6 hours, so the cluster is provisioned only once and is always available for subsequent executions. For more relaxed scenarios that don&#39;t require an hourly or near real time frequency, cluster can be dropped off much sooner by setting a smaller &#39;timetolive&#39; value, say 1 hour, for cost saving. More information on configuring time to live for on-demand cluster can be found [here](https://azure.microsoft.com/en-us/documentation/articles/data-factory-compute-linked-services/).

### Cluster size for on-demand HDInsight cluster

Depending on workload, you can choose size of on-demand HDInsight cluster. By default, the solution deploys a 4 node HDInsight cluster which is our recommendation for near real time scenarios, but for smaller workloads that run less often, you can try to reduce the number of worker nodes and see if the results are being generated within acceptable time delay based on your requirement. Read more about setting cluster size in Azure Data Factory [here](https://azure.microsoft.com/en-us/documentation/articles/data-factory-compute-linked-services/).

## More Information

Details on pricing for these individual services can be found [here](https://azure.microsoft.com/en-us/pricing)

For usage amounts and billing details for your subscription login to [Azure Management Portal](https://portal.azure.com/)

For estimating cost based on usage, please see [Azure Price Calculator](https://azure.microsoft.com/en-us/pricing/calculator/)
